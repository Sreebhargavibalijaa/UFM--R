# ğŸ¤– UFMÂ²-R: Unified Federated Multi-Modal Foundation Model with Interpretable Reasoning

**FedMM-X** is a federated learning framework built for **multi-modal data** â€” combining **image (MNIST)**, **text (IMDB)**, and **tabular** features â€” with a focus on privacy, trust, and interpretability.

## ğŸ” Key Features

- âœ… **Privacy-Preserving Learning**: Data never leaves the client.
- ğŸ§  **Multi-Modal Model**: Unified neural network processes image, text, and tabular inputs.
- ğŸ¯ **Trust-Weighted Aggregation**: Clients contribute based on reliability scores.
- ğŸ“ˆ **FedAvg vs. FedMM-X**: Compare aggregation strategies and accuracy across clients.
- ğŸ’¬ **LIME Interpretability**: Generate HTML explanations showing which tabular features influenced predictions.

## ğŸŒ Project Workflow

1. Load and align MNIST + IMDB data into multi-modal format.
2. Split data across clients with assigned trust levels.
3. Train using both **FedAvg** and **FedMM-X** methods.
4. Evaluate client accuracy and explainability using **LIME**.
5. Visualize results with comparison plots and explanation dashboards.

## ğŸ“Š Live Visualization

A **dynamic global graph** tracks each clientâ€™s accuracy and interpretability contribution during training. This helps identify which clients drive global performance and where explanations align or diverge.

> âš¡ Coming soon: Streamlit dashboard for real-time federated interpretability!

## ğŸ“ Outputs

- `checkpoints/` â€“ Saved FedAvg and FedMM-X models
- `lime_explanation_client_*.html` â€“ Feature attribution visualizations per client
- `accuracy_comparison.png` â€“ Bar chart comparing FedAvg and FedMM-X

## ğŸ› ï¸ Requirements

```bash
pip install torch torchvision flwr datasets lime matplotlib
